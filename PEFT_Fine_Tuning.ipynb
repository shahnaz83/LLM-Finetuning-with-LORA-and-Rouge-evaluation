{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Perform Parameter Efficient Fine-Tuning (PEFT)**\n"
      ],
      "metadata": {
        "id": "DxYM1njjYzzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now install the required packages for the LLM and datasets."
      ],
      "metadata": {
        "id": "v0HSBSxeZ1eS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nl5gOu5F3Qk"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pip install \\\n",
        "    transformers \\\n",
        "    datasets \\\n",
        "    evaluate \\\n",
        "    rouge_score\\\n",
        "    loralib \\\n",
        "    peft --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "onXBW9uTGP_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset and LLM\n",
        "We are going to continue experimenting with the DialogSum Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics."
      ],
      "metadata": {
        "id": "vtJ8YuE9aDcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "dataset = load_dataset(huggingface_dataset_name)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIt65mgsGRAw",
        "outputId": "6d3744cc-a664-4c15-cff2-a84274feae59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pre-trained FLAN-T5-small model and its tokenizer directly from HuggingFace."
      ],
      "metadata": {
        "id": "Rj1vrEpwagOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name='google/flan-t5-small'\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "SJgq3QhwPog7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it."
      ],
      "metadata": {
        "id": "n_wPpwxgar48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    all_model_params = model.num_parameters()\n",
        "    trainable_model_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
        "\n",
        "    percentage_trainable = 100 * trainable_model_params / all_model_params if all_model_params > 0 else 0\n",
        "\n",
        "    return (f\"Trainable model parameters: {trainable_model_params}\\n\"\n",
        "            f\"All model parameters: {all_model_params}\\n\"\n",
        "            f\"Percentage of trainable model parameters: {percentage_trainable:.2f}%\")\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(original_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1eOupt_Grpe",
        "outputId": "099ec09d-1d36-4662-e2ca-7c3da90bb28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable model parameters: 76961152\n",
            "All model parameters: 76961152\n",
            "Percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Preprocess the Dialog-Summary Dataset\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "You need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with Summarize the following conversation and to the start of the summary with Summary as follows:\n",
        "\n",
        "Training prompt (dialogue):\n",
        "\n",
        "Summarize the following conversation.\n",
        "\n",
        "    Chris: This is his part of the conversation.\n",
        "    Antje: This is her part of the conversation.\n",
        "    \n",
        "Summary:\n",
        "Training response (summary):\n",
        "\n",
        "Both Chris and Antje participated in the conversation.\n",
        "###Then preprocess the prompt-response dataset into tokens and pull out their input_ids (1 per token)."
      ],
      "metadata": {
        "id": "bQHam_SGblaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "    end_prompt = '\\n\\nSummary: '\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
      ],
      "metadata": {
        "id": "b5d8vmFpICuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup the PEFT/LoRA model for Fine-Tuning\n",
        "You need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained."
      ],
      "metadata": {
        "id": "AuwF-EWea6fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5-small\n",
        ")"
      ],
      "metadata": {
        "id": "BX8JWXXnG2iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add LoRA adapter layers/parameters to the original LLM to be trained."
      ],
      "metadata": {
        "id": "TNcOTj2_cPaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(original_model,\n",
        "                            lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pazi8u7YHZIh",
        "outputId": "b8ab5f04-daba-4163-f4a9-2d5dec7ec1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable model parameters: 1376256\n",
            "All model parameters: 78337408\n",
            "Percentage of trainable model parameters: 1.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "MQwZiGfmL_x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train PEFT Adapter\n",
        "Define training arguments and create Trainer instance."
      ],
      "metadata": {
        "id": "F-scNLkNcWgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=20,\n",
        "    max_steps=10,\n",
        "    label_names=['labels']\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lDEDC4NHcR2",
        "outputId": "290055fb-2c4a-4d1a-bb93-f18c02abd238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now everything is ready to train the PEFT adapter and save the model."
      ],
      "metadata": {
        "id": "eXMEcLYxcfFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "peft_trainer.train()\n",
        "\n",
        "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "X3tg2tW4HwHu",
        "outputId": "0df670d6-4c74-4742-9a24-0db8d469c966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:07, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
              " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
              " './peft-dialogue-summary-checkpoint-local/spiece.model',\n",
              " './peft-dialogue-summary-checkpoint-local/added_tokens.json',\n",
              " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare this model by adding an adapter to the original FLAN-T5-small model. We are setting is_trainable=False because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set is_trainable=True."
      ],
      "metadata": {
        "id": "lMdI2j1Ccz-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
        "                                       './peft-dialogue-summary-checkpoint-local',\n",
        "                                       torch_dtype=torch.bfloat16,\n",
        "                                       is_trainable=False)"
      ],
      "metadata": {
        "id": "ah7s8W7cPbmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of trainable parameters will be 0 due to is_trainable=False setting:"
      ],
      "metadata": {
        "id": "Er8WtO0ldJHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW4uTLDBM7lV",
        "outputId": "ecb9c664-1aea-49e1-bc57-53f538a0f9f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable model parameters: 0\n",
            "All model parameters: 78337408\n",
            "Percentage of trainable model parameters: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Evaluate the Model Quantitatively (with ROUGE Metric)\n",
        "Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time)."
      ],
      "metadata": {
        "id": "IpCD3WqPc-gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "rouge = load(\"rouge\")\n",
        "original_model = original_model.to(\"cuda\")\n",
        "peft_model = peft_model.to('cuda')\n",
        "# 1) گرفتن 10 دیالوگ تست و 10 خلاصه انسانی\n",
        "test_dialogues = dataset[\"test\"][0:10][\"dialogue\"]\n",
        "reference_summaries = dataset[\"test\"][0:10][\"summary\"]\n",
        "# ۱) گرفتن خلاصه‌ها از مدل\n",
        "predictions_original = []\n",
        "for dialogue in test_dialogues:\n",
        "    inputs = tokenizer(dialogue, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = original_model.generate(**inputs, max_new_tokens=200)\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    predictions_original.append(summary)\n",
        "\n",
        "predictions_peft = []\n",
        "for dialogue in test_dialogues:\n",
        "    inputs = tokenizer(dialogue, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = peft_model.generate(**inputs, max_new_tokens=200)\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    predictions_peft.append(summary)\n",
        "\n",
        "\n",
        "\n",
        "scores_original = rouge.compute(\n",
        "    predictions=predictions_original,\n",
        "    references=reference_summaries,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "scores_peft = rouge.compute(\n",
        "    predictions=predictions_peft,\n",
        "    references=reference_summaries,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print(\"ORIGINAL MODEL:\")\n",
        "print(scores_original)\n",
        "\n",
        "print(\"PEFT MODEL:\")\n",
        "print(scores_peft)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqpQeJPDUJ8A",
        "outputId": "7d4f66be-ff6b-47ad-c4eb-5b74ec518de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': np.float64(0.14700941637677367), 'rouge2': np.float64(0.027025045396923287), 'rougeL': np.float64(0.1373872208920477), 'rougeLsum': np.float64(0.1370316385333234)}\n",
            "PEFT MODEL:\n",
            "{'rouge1': np.float64(0.09365009990009993), 'rouge2': np.float64(0.006666666666666666), 'rougeL': np.float64(0.07498479330375882), 'rougeLsum': np.float64(0.07545451036830347)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the improvement of PEFT over the original model:"
      ],
      "metadata": {
        "id": "8ehpzzplgTgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" percentage improvement of PEFT MODEL:\")\n",
        "for metric in scores_original:\n",
        "    diff = scores_peft[metric] - scores_original[metric]\n",
        "    print(f\"{metric}: {diff * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCUw2PQ1tf-g",
        "outputId": "e0e5c731-2e05-461c-8245-7ffb3fcc222c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " percentage improvement of PEFT MODEL:\n",
            "rouge1: -5.34%\n",
            "rouge2: -2.04%\n",
            "rougeL: -6.24%\n",
            "rougeLsum: -6.16%\n"
          ]
        }
      ]
    }
  ]
}